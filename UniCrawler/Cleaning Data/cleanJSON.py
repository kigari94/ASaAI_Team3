# Python program to READ and CLEAN the output files
# generated by various spiders
# JSON file - creates a new file to keep raw data

import json
import os
import re
import spacy

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

# # einmaliges Ausf√ºhren
# nltk.download('punkt')
# nltk.download('stopwords')
# nltk.download('wordnet')
# nltk.download('omw-1.4')
# nltk.download('averaged_perceptron_tagger')

# one via console: spacy download de_core_news_sm
# load the german language model of spacy
nlp = spacy.load("de_core_news_sm")

# initiates the list with additional words to filter
stopList = "additional_words.json"

def clean_string(input):
    if input is not None:
        input = input.strip()
        text = re.sub(r'[^\w\s]|[\d]', '', str(input))
        lowercase_tokens = [token.lower() for token in word_tokenize(text, language='german')]
        return lowercase_tokens
    else:
        return ''


def clean_json(fname):
    content = list()
    # open file
    try:
        with open(fname) as f:
            data = json.load(f)

    except Exception as e:
        print(f"When opening {fname} an error occured.")
        print(f"Error: {e}")

    # check json format
    if type(data) is list:
        print(f"{fname} is in the correct format")

        for i in data:
            # if type(i) is dict:
            if isinstance(i, dict):
                helpList = list()
                helpDict = dict()
                for e in i['paragraphs']:
                    # .strip() to remove whitespace
                    helpList.append(e.strip())

                # RegEX to further clean
                text = re.sub(r'[^\w\s]|[\d]', '', str(helpList))

                # trained nltk wordtokenize, creates lowercase words out of whole sentences
                lowercase_tokens = [token.lower() for token in word_tokenize(text, language='german')]

                # clear title
                something = clean_string(i['title'])

                # StopWords are unnecessary fill words
                stop_words = set(stopwords.words("german"))
                additional_words = list()

                # additional_words is a list with further words we'd like to remove from the texts
                with open(stopList) as f:
                    additional_words = json.load(f)

                stop_words.update(additional_words)

                # creates new list with filtered words
                try:
                    filtered_tokens = [token for token in lowercase_tokens if token not in stop_words]
                except Exception as e:
                    print(f"Error: {e}")

                combined_string = ' '.join(filtered_tokens)
                doc = nlp(combined_string)

                # remove (Adverben, Determinante, ADP und Pronomen)
                removable_tags = ['ADV', 'DET', 'ADP', 'PRON']
                filtered_tokens = [token.text for token in doc if token.pos_ not in removable_tags]
                #print(filtered_tokens)

                filtered_string = ' '.join(filtered_tokens)
                filtered_doc = nlp(filtered_string)

                tokens = list()
                for chunk in filtered_doc.noun_chunks:
                    tokens.append(chunk.text)

                tokens.append(' '.join(something))

                helpDict['stud_url'] = i['stud_url']
                helpDict['title'] = i['title']
                helpDict['paragraphs'] = tokens

                content.append(helpDict)

            else:
                print(f"Element is not a dict: {type(i)}")
    else:
        print(f"{fname} is not a valid JSON.")

    if content is not None:
        write_json("../Resources/Cleaned/" + os.path.basename(fname).split(".")[0] + "_cleaned.json", content)
    else:
        print(f"Content is empty, something went wrong with: {fname}")


def write_json(fname, content):
    with open(fname, 'w') as f:
        json.dump(content, f)
        print(f"JSON file {fname} created successfully")


# theoretisch als funktion
# loads json files from the following directory '../Resources' and no further directories
for root, dirs, files in os.walk('../Resources'):
    if root == "../Resources":
        for file in files:
            # check for json file
            if (file.endswith('.json')):
                fname = root + '/' + file
                clean_json(fname)
            else:
                print(f"{file} has no json extension.")
    print("finished, no more json files")
