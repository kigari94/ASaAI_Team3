# Python program to READ and CLEAN the output files
# generated by various spiders
# JSON file - creates a new file to keep raw data

import json
import os
import re
import spacy

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

# # einmaliges Ausführen
# nltk.download('punkt')
# nltk.download('stopwords')
# nltk.download('wordnet')
# nltk.download('omw-1.4')
# nltk.download('averaged_perceptron_tagger')

# one via console: spacy download de_core_news_sm
# load the german language model of spacy
nlp = spacy.load("de_core_news_sm")

# initiates the list with additional words to filter
stopList = "additional_words.json"

def clean_string(input):
    if input is not None:
        input = input.strip()
        text = re.sub(r'[^\w\s]|[\d]', '', str(input))
        lowercase_tokens = [token.lower() for token in word_tokenize(text, language='german')]
        return lowercase_tokens
    else:
        return ''

def filter_noncourses(content):
    # defining terms for filtering the courses
    # depending on their title
    terms = {"Studieng\u00e4nge", "ǀ Bachelorstudiengänge", "Bachelorstudiengänge", "Studienangebot", "Masterabschluss",
             "Modulhandbücher", "Modulhandbuch", "ǀ Akademische Abschlüsse", "Abschlüsse", "Abschlüsse", "Colloquium",
             "Kolloquium", "Fachsemester", "Prüfungsordnungen", "Weiterbildende Masterstudengänge", "Liste",
             "Bewerbung_Verweis_Sommersemester", "Überblick", "International Office", "Studierenden-Vertretungen",
             "Schnuppertage", "Studienfonds", "Wintersemester", "Bewerbungsportal", "SS 19", "WS 19", "SS 20", "WS 20",
             "Studiengangsordnungen", "Warum"
             }
    count = 0

    # get courses
    for j, c in enumerate(content):
        # finding the title
        value = c['title']
        # check if title is empty
        if value is not None:
            for term in terms:
                # check if title contains term
                if term in value:
                    # print('Deleting: ' + str(term) + " Nummer: " + str(j) + " Value: " + str(
                    #     value) + " from list: " + str(content[j]))
                    # delete wrong courses
                    del content[j]
                    count += 1
                    break
    print('Deleted Objects: ' + str(count))

def filter_duplicates(content):
    memory = list()
    count = 0

    for j, c in enumerate(content):
        # finding the title
        value = c['title']
        # check if title is empty
        if value is not None:
            # Check for duplicates
            if value in memory:
                # print(str(value) + " from list: " + str(content[j]))
                # delete duplicates
                del content[j]
                count += 1
            else:
                memory.append(value)
    print('Duplicates Removed: ' + str(count))

def clean_json(fname):
    content = list()
    # open file
    try:
        with open(fname, encoding="utf-8") as f:
            data = json.load(f)

    except Exception as e:
        print(f"When opening {fname} an error occured.")
        print(f"Error: {e}")

    # check json format
    if type(data) is list:
        print(f"{fname} is in the correct format")

        for i in data:
            # if type(i) is dict:
            if isinstance(i, dict):
                helpList = list()
                helpDict = dict()
                for e in i['paragraphs']:
                    # .strip() to remove whitespace
                    helpList.append(e.strip())

                # RegEX to further clean
                text = re.sub(r'[^\w\s]|[\d]', '', str(helpList))

                # trained nltk wordtokenize, creates lowercase words out of whole sentences
                lowercase_tokens = [token.lower() for token in word_tokenize(text, language='german')]

                # clear title
                something = clean_string(i['title'])

                # StopWords are unnecessary fill words
                stop_words = set(stopwords.words("german"))
                additional_words = list()

                # additional_words is a list with further words we'd like to remove from the texts
                with open(stopList) as f:
                    additional_words = json.load(f)

                stop_words.update(additional_words)

                # creates new list with filtered words
                try:
                    filtered_tokens = [token for token in lowercase_tokens if token not in stop_words]
                except Exception as e:
                    print(f"Error: {e}")

                combined_string = ' '.join(filtered_tokens)
                doc = nlp(combined_string)

                # remove (Adverben, Determinante, ADP und Pronomen)
                removable_tags = ['ADV', 'DET', 'ADP', 'PRON', 'X', 'ADJ']
                filtered_tokens = [token.text for token in doc if token.pos_ not in removable_tags]
                #print(filtered_tokens)

                filtered_string = ' '.join(filtered_tokens)
                filtered_doc = nlp(filtered_string)

                tokens = list()
                for chunk in filtered_doc.noun_chunks:
                    tokens.append(chunk.text)

                tokens.append(' '.join(something))

                helpDict['stud_url'] = i['stud_url']
                helpDict['title'] = i['title']
                helpDict['paragraphs'] = tokens

                content.append(helpDict)

            else:
                print(f"Element is not a dict: {type(i)}")
    else:
        print(f"{fname} is not a valid JSON.")

    filter_noncourses(content)
    filter_duplicates(content)

    if content is not None:
        write_json("../Resources/Cleaned/" + os.path.basename(fname).split(".")[0] + "_cleaned.json", content)
    else:
        print(f"Content is empty, something went wrong with: {fname}")


def write_json(fname, content):
    with open(fname, 'w') as f:
        json.dump(content, f, indent=4)
        print(f"JSON file {fname} created successfully")


# theoretisch als funktion
# loads json files from the following directory '../Resources' and no further directories
for root, dirs, files in os.walk('../Resources'):
    if root == "../Resources":
        for file in files:
            # check for json file
            if (file.endswith('.json')):
                fname = root + '/' + file
                clean_json(fname)
            else:
                print(f"{file} has no json extension.")
    print("finished, no more json files")
